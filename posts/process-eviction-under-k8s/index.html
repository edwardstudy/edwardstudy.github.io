<!doctype html><html lang=en dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>k8s单机侧的驱逐策略总结 | Edwardesire</title><meta name=generator content="Hugo Eureka 0.9.3"><link rel=stylesheet href=/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css><script defer src=/js/eureka.min.e30461307d4134a93cbbd7301a29631abc285b8d3df327389368cd910c20926df5759064caf0f8ba686f65edcbdff5ee.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap" as=style onload="this.onload=null,this.rel='stylesheet'"><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/solarized-light.min.css media=print onload="this.media='all',this.onload=null" crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js crossorigin></script>
<link rel=stylesheet href=/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css media=print onload="this.media='all',this.onload=null"><script defer type=text/javascript src=/js/fontawesome.min.7b735802cbde01152678cc4a867f0b4bb0597876e29adfcaf1e5b5e99d39d7af87f348fdfd91c7eaf9ed7dc1eece7c61.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ media=print onload="this.media='all',this.onload=null" crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js integrity=sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0 crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=/images/favicon-32x32_hub39b788047e733a7d94c242c3bed8659_782_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=/images/favicon-32x32_hub39b788047e733a7d94c242c3bed8659_782_180x180_fill_box_center_3.png><meta name=description content="本文从k8s出发，总结不同层次下的驱逐流程和进程选择策略。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":2,"name":"k8s单机侧的驱逐策略总结","item":"/posts/process-eviction-under-k8s/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/process-eviction-under-k8s/"},"headline":"k8s单机侧的驱逐策略总结 | Edwardesire","datePublished":"2022-06-07T14:27:08+08:00","dateModified":"2022-06-07T14:27:08+08:00","wordCount":10954,"publisher":{"@type":"Person","name":"C. Wang","logo":{"@type":"ImageObject","url":"/images/favicon-32x32.png"}},"description":"本文从k8s出发，总结不同层次下的驱逐流程和进程选择策略。"}</script><meta property="og:title" content="k8s单机侧的驱逐策略总结 | Edwardesire"><meta property="og:type" content="article"><meta property="og:image" content="/images/favicon-32x32.png"><meta property="og:url" content="/posts/process-eviction-under-k8s/"><meta property="og:description" content="本文从k8s出发，总结不同层次下的驱逐流程和进程选择策略。"><meta property="og:locale" content="en"><meta property="og:site_name" content="Edwardesire"><meta property="article:published_time" content="2022-06-07T14:27:08+08:00"><meta property="article:modified_time" content="2022-06-07T14:27:08+08:00"><meta property="article:section" content="posts"><meta property="article:tag" content="eviction"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="memory-management"><meta property="article:tag" content="Linux"><meta property="og:see_also" content="/posts/the-trending-of-cpu-management-in-k8s/"><meta property="og:see_also" content="/posts/node-resource-topology-aware-scheduling/"><meta property="og:see_also" content="/posts/numa-aware-scheduling-in-volcano/"><meta property="og:see_also" content="/posts/k8s-resources-topology-manager/"><meta property="og:see_also" content="/posts/multi-cluster-scheduling/"><meta property="og:see_also" content="/posts/load-scheduling-brief/"><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");((storageColorScheme=='Auto'||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName('html')[0].classList.add('dark')</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="me-6 text-primary-text text-xl font-bold">Edwardesire</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Posts</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById('lightDarkMode');storageColorScheme==null||storageColorScheme=='Auto'?document.addEventListener('DOMContentLoaded',()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change',switchDarkMode)}):storageColorScheme=="Light"?(element.firstElementChild.classList.remove('fa-adjust'),element.firstElementChild.setAttribute("data-icon",'sun'),element.firstElementChild.classList.add('fa-sun')):storageColorScheme=="Dark"&&(element.firstElementChild.classList.remove('fa-adjust'),element.firstElementChild.setAttribute("data-icon",'moon'),element.firstElementChild.classList.add('fa-moon')),document.addEventListener('DOMContentLoaded',()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8"><div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12"><div class="lg:col-start-2 bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"><article class=prose><h1 class=mb-4>k8s单机侧的驱逐策略总结</h1><div class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"><div class="me-6 my-2"><i class="fas fa-calendar me-1"></i>
<span>2022-06-07</span></div><div class="me-6 my-2"><i class="fas fa-clock me-1"></i>
<span>22 min read</span></div><div class="me-6 my-2"><i class="fas fa-folder me-1"></i>
<a href=/categories/eviction/ class=hover:text-eureka>eviction</a>
<span>,</span>
<a href=/categories/kubernetes/ class=hover:text-eureka>Kubernetes</a>
<span>,</span>
<a href=/categories/memory-management/ class=hover:text-eureka>memory-management</a>
<span>,</span>
<a href=/categories/linux/ class=hover:text-eureka>Linux</a></div></div><p>进程驱逐：当机器存在资源压力时，可能是由于有恶意程序在消耗系统资源，或者是overcommit导致。系统通过控制机器上的进程存活来减少单个程序对系统的整体影响。驱逐阶段最关键的就是选择合适的进程，通过最小代价来保证系统的稳定。在执行层面上可以分为两种驱逐方式：</p><ul><li>用户空间驱逐：通过守护进程之类的机制，触发式主动清理进程。</li><li>内核空间驱逐：内核在无法分配内存时，通过oom_killer选择进程终止来释放资源。</li></ul><p>本文从k8s出发，总结不同层次下的驱逐流程和进程选择策略。</p><h2 id=kubelet驱逐策略>Kubelet驱逐策略</h2><p>k8s除了支持API发起的主动驱逐，还支持用户空间的pod驱逐（将资源大户的进程终止）。对于不可压缩资源：内存、disk（nodefs）、pid，kubelet会监控相应的指标来触发pod驱逐。K8S依据pod的资源消耗和优先级来驱逐pod来回收资源：</p><ul><li>如果pod资源使用量超过资源请求值，则优先驱逐</li><li>依据pod priority驱逐</li><li>pod真实资源使用量越高则越优先驱逐</li></ul><p>我们可以得出：</p><ul><li>当BestEffort和Burstable pod的资源使用量超过请求值时，会依据pod priority和超过请求多少来判断驱逐顺序。也不会有特例的pod能够不被驱逐的风险。当Guaranteed和Busrtable pod的使用量低于请求值时，基于pod priority确定驱逐顺序。</li></ul><p>这一切的逻辑都在kubelet的eviction manager实现。</p><h3 id=eviction-manager>Eviction manager</h3><p>Manager的接口定义包含主流程的启动函数以及提供给kubelet上报节点状态的：</p><ul><li>Start()：开始驱逐控制循环，获取监控数据，并判断资源是否到阈值，触发pod的驱逐，以及当节点出现压力时将本地的节点状态更新。</li><li>IsUnderMemoryPressure()：判断节点是否达到内存限制压力，通过控制循环内更新的节点状态判断。</li><li>IsUnderDiskPressure()：判断节点是否达到磁盘限制压力，通过控制循环内更新的节点状态判断。</li><li>IsUnderPIDPressure()：判断节点是否达到PID限制压力，通过控制循环内更新的节点状态判断。</li></ul><p>kubelet在tryUpdateNodeStatus上报节点状态循环中，会调用上述方法判断节点的资源压力情况。</p><p>kubelet在初始化evictionManager之后会调用evictionManager.Start()启动驱逐，之后再同步节点状态时调用上述压力判断方法。除了实现Manager的接口外，还实现了在pod生命周期负责评估允许pod执行的PodAdmitHandler接口。evictionManager主要是依据pod的性质判断是否能够在已经有资源压力的机器上创建容器。</p><h3 id=驱逐控制循环>驱逐控制循环</h3><h4 id=初始化阶段>初始化阶段</h4><p>kubelet主程解析配置并初始化evictionManager，解析单机的资源阈值参数ParseThresholdConfig()</p><p>kubelet以signal维度设定资源阈值，每个signal标识一种资源指标，定义资源的阈值和其他驱逐参数。比如<code>memory.available</code>表示节点可用内存驱逐标识（memory.available = capacity - workingSet）。</p><p>kubelet通过下列参数确定资源signal属性，构建相应资源的阈值。</p><ul><li><code>--eviction-hard mapStringString</code>：资源驱逐硬下线，默认为：imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10%</li><li><code>--eviction-soft mapStringString</code>：资源驱逐的软下线，当触发时，pod有优雅退出时间。</li><li><code>--eviction-soft-grace-period mapStringString</code>：触发黄线时，pod驱逐的优雅退出时间。</li><li><code>--eviction-minimum-reclaim mapStringString</code>：资源的最小释放量。默认为0。</li></ul><p>其中，同一个资源的eviction-soft和soft-grace-period配置必须都存在，<code>grace period must be specified for the soft eviction threshold</code>。</p><p>通过解析配置项，设置各资源signal的阈值之后，kubelet调用evictionManager.Start()驱动evictionManager工作。</p><h4 id=evictionmanager的启动>evictionManager的启动</h4><p>在启动控制循环之前，evictionManager会增加对cgroup内存子系统监控的预处理。这个预处理通过<strong>cgroup notifier</strong>的机制监听mem cgroup的使用情况，并且在控制循环中周期性更新cgroup notifier的阈值配置。</p><h5 id=memorythresholdnotifier>MemoryThresholdNotifier</h5><p>evictionManager分别给<code>memory.available</code>和<code>allocatableMemory.available</code> signal配置MemoryThresholdNotifier的，监控的cgroup路径不同。<code>allocatableMemory.available</code>的cgroupRoot根目录，即节点上pods的根cgroup。<code>memory.available</code>则监控<code>/proc/cgroups/memory</code>目录。</p><p>MemoryThresholdNotifier的工作流程是：</p><ul><li><p>初始化MemoryThresholdNotifier</p><p>MemoryThresholdNotifier需要获取cgroup目录的cgoup内存子系统路径，并设置evictionManager.synchronize()为阈值处理函数thresholdHandler</p></li><li><p>创建goroutine来启动MemoryThresholdNotifier</p><p>在MemoryThresholdNotifier.Start()循环中：监听事件channel，并调用驱逐函数（调用synchronize）</p></li><li><p>在synchronize阶段调用UpdateThreshold()更新memcg的阈值，并激活MemoryThresholdNotifier。</p><p>依据当前的采集指标配置，计算cgroup 内存使用阈值。</p><p>如果MemoryThresholdNotifier已经存在notifier实例，则创建新的cgroupNotifier替换。cgroupNotifier通过epoll上述eventfd描述符的方式，监听内存超过阈值的事件。</p></li></ul><p>这里有两个关键点：</p><ol><li>在UpdateThreshold函数中计算cgroup内存使用阈值</li></ol><p>如上述，通过监听memory.usage_in_bytes文件，获取内存使用情况（不包含swap），当内存使用阈值。而内存使用阈值 <strong>memcgThreshold</strong> 通过监控数据得来：</p><pre><code>	// Set threshold on usage to capacity - eviction_hard + inactive_file,
	// since we want to be notified when working_set = capacity - eviction_hard
	inactiveFile := resource.NewQuantity(int64(*memoryStats.UsageBytes-*memoryStats.WorkingSetBytes), resource.BinarySI)
	capacity := resource.NewQuantity(int64(*memoryStats.AvailableBytes+*memoryStats.WorkingSetBytes), resource.BinarySI)
	evictionThresholdQuantity := evictionapi.GetThresholdQuantity(m.threshold.Value, capacity)
	memcgThreshold := capacity.DeepCopy()
	memcgThreshold.Sub(*evictionThresholdQuantity)
	memcgThreshold.Add(*inactiveFile)
</code></pre><p>计算内存使用阈值 <strong>memcgThreshold</strong> 的绝对值通过 capacity - eviction_hard（如果红线不为绝对值，则依据capacity * 百分比） + inactive_file 计算而来。</p><p>其中</p><ul><li>内存容量capacity = memoryStats.AvailableBytes + memoryStats.WorkingSetBytes，即 内存可用量 + workload已使用量（两个值都从监控模块得到）</li><li>硬下线eviction_hard是参数值</li><li>不活跃的文件内存页 inactive_file = memoryStats.UsageBytes - memoryStats.WorkingSetBytes，即 内存已使用量 - workload已使用量（包含最近使用的内存、待回收的脏内存和内核占用内存，两个值也从监控模块得到）。</li></ul><ol start=2><li>在UpdateThreshold函数中创建cgroupNotifier</li></ol><p><strong>cgroup notifier</strong>的机制是通过eventfd监听cgroup中内存使用超过阈值的事件。</p><ul><li>memory.usage_in_bytes：监听内存使用文件对象。</li><li>cgroup.event_control：阈值监控控制接口，依据 <code>&lt;event_fd> &lt;fd of memory.usage_in_bytes> &lt;threshold></code>的格式配置event_fd，watchfd和阈值threshold。</li></ul><pre><code>/sys/fs/cgroup/memory
# cat memory.usage_in_bytes
92459601920
# ls -lt cgroup.event_control
--w--w--w- 1 root root 0 Nov 24 12:05 cgroup.event_control     # an interface for event_fd()
</code></pre><p>cgroupNotifier会依据cgroup事件向channel压入事件，触发事件消费者（evictionManager）处理。这里channel不会传递具体的事件内容，只做任务触发功能。</p><p>注册cgroup的threshold，需要有3步：</p><ul><li>使用eventfd(2)创建eventfd</li><li>创建打开memory.usage_in_bytes或者memory.memsw.usage_in_bytes文件描述符</li><li>在cgroup.event_control写入"&lt;event_fd> <fd of memory.usage_in_bytes><threshold>" 信息</li></ul><p>在evictionManager.Start()的最后启动控制循环synchronize周期性检查驱逐的阈值条件是否达到，并进行下一步动作。</p><h4 id=控制循环synchronize>控制循环synchronize</h4><p>在evictionManager的控制循环中，维持10s调用synchronize函数来选择pod驱逐。驱逐首要判断的就是驱逐的触发条件，通过监控系统资源的方式来判断资源使用情况是否触及阈值。evictionManager有两种触发方式：</p><ol><li><p>基于cgroup触发驱逐（基于事件）：上述已经描述了内存的CgroupNotifier机制</p></li><li><p>依据监控数据触发驱逐（周期性检查）</p><p>2.1 通过summaryProvider获取节点和pods的资源使用情况</p><p>2.2 在signalObservations函数中依据监控数据，获取各资源的使用情况signalObservations</p><p>​ 单个signalObservation记录着资源的总量和可用量：</p><pre><code>// signalObservation is the observed resource usage
type signalObservation struct {
	// The resource capacity
	capacity *resource.Quantity
	// The available resource
	available *resource.Quantity
	// Time at which the observation was taken
	time metav1.Time
}
</code></pre><p>2.3 在thresholdsMet函数中判断是否需要驱逐来释放资源</p><p>​ 当上述观测到的资源可用量低于各signal的阈值时，返回需要释放的资源类型。</p></li></ol><p>无论哪种方式，都会执行synchronize后段逻辑来判断是否需要驱逐pod。</p><ol start=3><li><p>更新节点状态，将资源压力状态更新，并上报到集群API</p><p>集群内其他组件能够观测到节点状态，从节点外部处理。</p></li><li><p>如果开启了featuregate LocalStorageCapacityIsolation本地存储，会首先尝试清理影响本地磁盘</p><p>这个是依据featuregate来控制是否开启，会检查pod下列资源使用是否超过limit值。</p><ul><li>emptyDir的sizeLimit</li><li>ephemeralStorage的limit</li><li>container的ephemeralStorage limit</li></ul></li></ol><p>这种驱逐是立即的，没有优雅退出时间。当触发到本地磁盘触发条件时，会忽略其他资源的驱逐行为。</p><p>当驱逐流程走到这，会判断是否存在资源紧张的驱逐资源。如果thresholdsMet返回的空数组，则表示没有资源触及到驱逐阈值。否则继续执行节点资源的回收。</p><ol start=5><li><p>回收节点级别的资源</p><p>5.1 reclaimNodeLevelResource：回收节点级别资源</p><p>首先尝试回收节点资源：nodefs/imagefs，这部分可以通过删除没使用的容器和镜像，而不侵害执行中的pod。调用完节点资源回收函数之后，再采集一次指标。如果空闲资源大于阈值，则跳过本次驱逐的后续流程：pod级别的驱逐。</p><p>5.2 rank阶段：判断触发驱逐条件的资源优先级</p><p>每次synchronize只会选择一个超过阈值的资源进行回收。当多个资源出现触碰到阈值时，资源驱逐优先级如下：</p><ul><li>内存资源的驱逐优先级最高</li><li>没有资源signal的优先级最低</li></ul><p>5.3 尝试回收用户pods的资源</p><p>依据上一个步骤获得的资源signal来判断节点上活跃pod的驱逐优先级，将pod依据驱逐优先级排序：</p><p>比如依据内存资源评判pod驱逐优先级规则有：</p><ul><li>依据pod是否超出资源请求值：没有资源使用指标的首先被驱逐。超过请求值的首先被驱逐。</li><li>依据pod的spec.priority：依据pod配置的优先级排序，默认为0。priority越高的pod，驱逐序列越靠后。</li><li>依据内存资源消费：依据pod消费内存超过请求值的部分排序。超过的资源绝对值越高的pod越优先被驱逐。</li></ul><p>kubelet实现了multiSorter的功能：依据上述顺序将活跃的pod排序。如果当前规则的结果是等序，才进入下个规则判断pod优先级。上述内存资源评判逻辑翻译过来就是，首先找出资源使用量超过请求值的pod（包含没有指标的pod），然后依据pod的spec.priority排序。在同priority的pod内部再依据超过的资源绝对值越高的pod排序。</p><p>除了rankMemoryPressure的逻辑，还有rankPIDPressure，rankDiskPressure的逻辑。</p><p>5.4 驱逐</p><p>在依据可回收资源的排序后，每次驱逐周期只会执行一次pod的删除。如果不是HardEviction，还会给MaxPodGracePeriodSeconds的时间来让pod内的容器进程退出。具体的驱逐动作操作在发送事件，删除pod并更新pod的驱逐状态。</p></li></ol><h2 id=系统驱逐策略>系统驱逐策略</h2><p>上面描述的是用户态中kubelet通过驱逐来限制节点资源、pod资源。在内核内存管理中，通过OOM killer来限制单机层面的内存使用。</p><h3 id=oom-killer>OOM killer</h3><p>OOM killer（Out Of Memory killer）是一种Linux内核的一种内存管理机制：在系统可用内存较少的情况下，内核为保证系统还能够继续运行下去，会选择结束进程来释放内存资源。</p><h4 id=运行机制>运行机制</h4><p>running processes require more memory than is physically available. 内核在调用alloc_pages()分配内存时，如果所需要的内存超过物理内存时，通过调用out_of_memory()函数来选择进程释放资源。OOM killer会检查所有运行中的进程，选择结束一个活多个进程来释放系统内存。</p><p>out_of_memory()函数：先做一部分检查，避免通过结束进程的方式来释放内存。如果只能通过结束进程的方式来释放，那么函数会继续选择目标进程来回收。如果这个阶段也无法释放资源，kernel最终报错异常退出。函数源码地址：https://elixir.bootlin.com/linux/v5.17.2/source/mm/oom_kill.c#L1052，流程如下：</p><ol><li>首先通知oom_notify_list链表的订阅者：依据通知链（notification chains）机制，通知注册了oom_notify_list的模块释放内存。如果订阅者能够处理OOM，释放了内存则会退出OOM killer，不执行后续操作。</li><li>如果当前task存在pending的SIGKILL，或者已经退出的时，会释放当前进程的资源。包括和task共享同一个内存描述符mm_struct的进程、线程也会被杀掉。</li><li>对于IO-less的回收，依据gfp_mask判断，如果1) 分配的是非FS操作类型的分配，并且2）不是cgroup的内存OOM -> 直接退出oom-killer。</li><li>检查内存分配的约束（例如NUMA），有CONSTRAINT_NONE, CONSTRAINT_CPUSET，CONSTRAINT_MEMORY_POLICY, CONSTRAINT_MEMCG类型。</li><li>检查<code>/proc/sys/vm/panic_on_oom</code>的设置，做操作。可能panic，也可能尝试oom_killer。如果panic_on_oom设置的为2，则进程直接panic强制退出。</li><li><code>/proc/sys/vm/oom_kill_allocating_task</code>为true的时候，调用oom_kill_process直接kill掉当前想要分配内存的进程(此进程能够被kill时)。</li><li>select_bad_process()，选择最合适的进程，调用oom_kill_process。</li><li>如果没有合适的进程，如果非sysrq和memcg，则panic强制退出。</li></ol><p>上述流程中有几个细节：</p><h5 id=gfp_mask约束>gfp_mask约束</h5><pre><code>	/*
	 * The OOM killer does not compensate for IO-less reclaim.
	 * pagefault_out_of_memory lost its gfp context so we have to
	 * make sure exclude 0 mask - all other users should have at least
	 * ___GFP_DIRECT_RECLAIM to get here. But mem_cgroup_oom() has to
	 * invoke the OOM killer even if it is a GFP_NOFS allocation.
	 */
	if (oc-&gt;gfp_mask &amp;&amp; !(oc-&gt;gfp_mask &amp; __GFP_FS) &amp;&amp; !is_memcg_oom(oc))
		return true;
</code></pre><p>gfp_mask是申请内存（get free page）时传递的标志位。前四位表示内存域修饰符（___GFP_DMA、___GFP_HIGHMEM、___GFP_DMA32、___GFP_MOVABLE），从第5位开始为内存分配标志。定义：https://elixir.bootlin.com/linux/v5.17.2/source/include/linux/gfp.h#L81。默认为空，从ZONE_NORMAL开始扫描，ZONE_NORMAL是默认的内存申请类型。</p><p>OOM killer不对非IO的回收进行补偿，所以分配的gfp_mask是非FS操作类型的分配的OOM会直接退出。</p><h5 id=oom_constraint约束>oom_constraint约束</h5><p>检查内存分配是否有限制，有几种不同的限制策略。仅适用于NUMA和memcg场景。oom_constraint可以是：CONSTRAINT_NONE,CONSTRAINT_CPUSET,CONSTRAINT_MEMORY_POLICY,CONSTRAINT_MEMCG类型。对于UMA架构而言，oom_constraint永远都是CONSTRAINT_NONE，表示系统并没有约束产生的OOM。而在NUMA的架构下，有可能附加其他的约束导致OOM的情况出现。</p><p>然后调用<code>check_panic_on_oom(oc)</code>检查是否配置了/proc/sys/kernel/panic_on_oom，如果有则直接触发panic。</p><p>当走到这一步，oom killer需要选择终止的进程，有两种选择逻辑选择合适的进程通过：</p><ul><li>谁触发OOM就终止谁：通过sysctl_oom_kill_allocating_task控制，是否干掉当前申请内存的进程</li><li>谁最“坏”就制止谁：通过打分判断最“坏”的进程</li></ul><p>sysctl_oom_kill_allocating_task来自<code>/proc/sys/vm/oom_kill_allocating_task</code>。当参数为true的时候，调用oom_kill_process直接kill掉当前想要分配内存的进程。</p><h5 id=select_bad_process选择最坏的进程>select_bad_process：选择最“坏”的进程</h5><p>普通场景下通过oom_evaluate_task函数，评估进程分数选择需要终止的进程。如果是memory cgroup的情况调用mem_cgroup_scan_tasks来选择。先看看oom_evaluate_task的逻辑</p><ul><li>mm->flags为MMF_OOM_SKIP的进程则跳过，遍历下一个进程评估</li><li>oom_task_origin分数最高，该标志表示task已经被分配大量内存并标记为oom的潜在原因，所以优先杀掉。</li><li>其他情况的进程通过oom_badness函数计算分数</li></ul><p>最后分数最高的进程被终止的优先级最高。</p><p>oom_badness函数计算的进程终止优先级<strong>分数</strong>由两部分组成，由下列两个参数提供。</p><p>参数：</p><ul><li>oom_score_adj：OOM kill score adjustment，调整值由用户打分。范围在 OOM_SCORE_ADJ_MIN（-1000） 到 OOM_SCORE_ADJ_MAX（1000）。数值越大，进程被终止的优先级越高。用户可以通过该值来保护某个进程。</li><li>totalpages：当前可分配的内存上限值，提供系统打分的依据。</li></ul><p>计算公式：</p><pre><code>	/*
	 * The baseline for the badness score is the proportion of RAM that each
	 * task's rss, pagetable and swap space use.
	 */
points = get_mm_rss(p-&gt;mm) + get_mm_counter(p-&gt;mm, MM_SWAPENTS) +
		mm_pgtables_bytes(p-&gt;mm) / PAGE_SIZE;
adj *= totalpages / 1000;
points += adj;
</code></pre><p>基础分数process_pages由3部分组成：</p><ul><li>get_mm_rss(p->mm)：rss部分</li><li>get_mm_counter(p->mm, MM_SWAPENTS)：swap占用内存</li><li>mm_pgtables_bytes(p->mm) / PAGE_SIZE：页表占用内存</li></ul><p>将3个部分相加，并结合oom_score_adj：将归一化后的adj和points求和，作为当前进程的分数。</p><p>所以进程得分points=process_pages + oom_score_adj*totalpages/1000</p><p>之前老的内核版本还会有一些复杂的计算逻辑考虑，比如对于特权进程的处理。如果是root权限的进程，有3%的内存使用特权。points=process_pages*0.97 + oom_score_adj*totalpages/1000。v4.17移除，使得计算逻辑更加简洁和可预测。</p><pre><code>/*
	 * Root processes get 3% bonus, just like the __vm_enough_memory()
	 * implementation used by LSMs.
	 */
	if (has_capability_noaudit(p, CAP_SYS_ADMIN))
		points -= (points * 3) / 100;
</code></pre><p>mem_cgroup_scan_tasks：memory cgroup cgroup的处理会需要遍历cgroup的层次结构，调用oom_evaluate_task计算task的分数。回收父进程的内存也会回收子进程的内存。</p><h6 id=oom_kill_process>oom_kill_process</h6><p>接下来进入终止进程的逻辑，oom_kill_process函数在终止进程之前会先检查，task是否已经退出，占用的内存会被释放，防止重复处理；获取memory cgroup消息，判断是否需要删除cgroup下所有的tasks。然后是dump信息，将OOM的原因打印出来，保留OOM的线索。</p><p>之后在__oom_kill_process函数内调用put_task_struct 释放内核栈，释放系统资源。唤醒oom_reaper内核线程收割wake_oom_reaper(victim)。</p><p>oom_reaper会在有清理任务之前一直保持休眠。wake_oom_reaper会将任务压入oom_reaper_list链表，oom_reaper通过oom_reaper_list链表判断需要调用oom_reap_task_mm清理地址空间。清理时会遍历vma，跳过VM_LOCKED|VM_HUGETLB|VM_PFNMAP的VMA区域。具体的释放操作通过unmap_page_range完成：</p><pre><code>	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {
		if (!can_madv_lru_vma(vma))
			continue;

		/*
		 * Only anonymous pages have a good chance to be dropped
		 * without additional steps which we cannot afford as we
		 * are OOM already.
		 *
		 * We do not even care about fs backed pages because all
		 * which are reclaimable have already been reclaimed and
		 * we do not want to block exit_mmap by keeping mm ref
		 * count elevated without a good reason.
		 */
		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED)) {
			struct mmu_notifier_range range;
			struct mmu_gather tlb;

			mmu_notifier_range_init(&amp;range, MMU_NOTIFY_UNMAP, 0,
						vma, mm, vma-&gt;vm_start,
						vma-&gt;vm_end);
			tlb_gather_mmu(&amp;tlb, mm);
			if (mmu_notifier_invalidate_range_start_nonblock(&amp;range)) {
				tlb_finish_mmu(&amp;tlb);
				ret = false;
				continue;
			}
			unmap_page_range(&amp;tlb, vma, range.start, range.end, NULL);
			mmu_notifier_invalidate_range_end(&amp;range);
			tlb_finish_mmu(&amp;tlb);
		}
	}
</code></pre><p><a href=https://elixir.bootlin.com/linux/v5.17.2/source/mm/oom_kill.c#L528>https://elixir.bootlin.com/linux/v5.17.2/source/mm/oom_kill.c#L528</a></p><h5 id=控制oom-killer的行为>控制oom killer的行为</h5><p>上述有提及几个文件参数来控制控制 oom killer的行为：</p><ol><li>/proc/sys/vm/panic_on_oom，当出现oom时，该值设定允许或者禁止kernel panic（默认为0）</li></ol><ul><li>0: 发生oom时，内核会选择调用oom-killer来选择进程删除</li><li>1: 发生oom时，内核通常情况会直接panic，除了特定条件：通过mempolicy/cpusets限制使用的进程则会被oom-killer删除时，不会panic</li><li>2: 发生oom时，内核无条件直接panic</li></ul><ol start=2><li><p>/proc/sys/vm/oom_kill_allocating_task，可以取值为0或者非0（默认为0），0代表发送oom时，进行遍历任务链表，选择一个进程去杀死，而非0代表，发送oom时，直接kill掉引起oom的进程，并不会去遍历任务链表。</p></li><li><p>/proc/sys/vm/oom_dump_tasks：可以取值为0或者非0（默认为1），表示是否在发送oom killer时，打印task的相关信息。</p></li><li><p>/proc/<pid>/oom_score_adj：配置进程的评分调整分，通过该在值来保护某个进程不被杀死或者每次都杀某个进程。其取值范围为-1000到1000 。</p></li><li><p>/proc/sys/vm/overcommit_memory：控制内存超售，oom-killer功能，默认为0</p></li></ol><ul><li>0： <strong>启发式策略</strong> ，比较严重的Overcommit将不能得逞，比如你突然申请了128TB的内存。而轻微的overcommit将被允许。另外，root能Overcommit的值比普通用户要稍微多。默认</li><li>1： <strong>永远允许overcommit</strong> ，这种策略适合那些不能承受内存分配失败的应用，比如某些科学计算应用。</li><li>2： <strong>永远禁止overcommit</strong> ，在这个情况下，系统所能分配的内存不会超过 <strong>swap+RAM*系数</strong> （/proc/sys/vm/overcmmit_ratio，默认50%，你可以调整），如果这么多资源已经用光，那么后面任何尝试申请内存的行为都会返回错误，这通常意味着此时没法运行任何新程序。</li></ul><p>Memory cgroup子系统的控制：</p><ol><li>memory.use_hierarchy：指定cgroup层次结构。（default为0）</li></ol><ul><li>0：父进程不从子进程回收内存</li><li>1：会从超出内存限制（memory limit）的子进程中回收</li></ul><ol start=2><li>memory.oom_control：oom控制，（默认为0：每个cgroup内存子系统）</li></ol><ul><li>0：当进程消费更多的内存时会被oom_killer杀掉</li><li>1：关闭oom_killer，当task尝试使用更多的内存时，会卡住直到内存充足。</li><li>读文件时，描述oom状态：oom_kill_disable（是否开启）、under_oom（是否处于oom状态）</li></ul><h2 id=用户空间的oom-killer>用户空间的oom killer</h2><p>最后再简单介绍一个用户空间的oom killer：https://github.com/facebookincubator/oomd。oomd的目标是在用户空间，解决内存资源使用的问题。</p><h3 id=运行机制-1>运行机制</h3><ul><li>使用PSI、cgroupv2来监控系统上的内存使用情况，oomd在内核的oom_killer处理之前，先进行内存资源的释放。</li><li>监控系统和cgroup的内存压力。</li></ul><p>并且配置上可以做到如此驱逐策略：</p><ul><li>当workload有内存压力/系统有内存压力时，通过内存大小或增长率选择一个memory hog（资源大户）删除。</li><li>当系统有内存压力时，通过内存大小或增长率选择一个memory hog 删除。</li><li>当系统有swap压力时，选择使用swap最多的cgroup来删除。</li></ul><p>可以看到，oomd充当了kubelet的功能，是单机上oom管理的agent。</p><hr><h2 id=总结>总结</h2><p>可以看到用户空间和内核空间的驱逐策略的不同。用户空间通过监控系统资源来触发驱逐流程，内核空间通过分配内存时触发驱逐流程。因为用户空间的驱逐需要在内核驱逐之前来</p><p>除了进程驱逐手段，还有其他手段来做到资源保障和稳定性，比如资源抑制和回收。通过cgroup v2的Memory Qos的能力</p><ul><li>当整机内存出现压力时，保障container的内存分配性能，降低其内存分配延迟</li><li>对过度申请内存的container进行抑制和快速回收，降低整机内存的使用压力</li><li>对整机保留内存进行保护</li></ul><h4 id=参考>参考</h4><ul><li>Memory Resource Controller: <a href=https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt>https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt</a></li><li>liux下oom killer: <a href=https://www.mo4tech.com/oom-killer-mechanism-for-the-linux-kernel.html>https://www.mo4tech.com/oom-killer-mechanism-for-the-linux-kernel.html</a></li><li>内存分配掩码（gfp_mask）：https://blog.csdn.net/farmwang/article/details/66975128</li><li>oom-killer日志分析：https://bhsc881114.github.io/2018/06/24/oom-killer%E7%90%86%E8%A7%A3%E5%92%8C%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/</li><li>memory managemnent：https://learning-kernel.readthedocs.io/en/latest/mem-management.html</li><li>Linux内存管理 (21)OOM：https://www.cnblogs.com/arnoldlu/p/8567559.html</li><li>Liux OOM的参数：http://www.wowotech.net/memory_management/oom.html</li></ul></article><div class=my-4><a href=/tags/eviction/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#eviction</a>
<a href=/tags/kubernetes/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Kubernetes</a>
<a href=/tags/memory-management/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#memory-management</a>
<a href=/tags/linux/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Linux</a></div><div class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"><div></div><div class="mt-4 md:mt-0 md:text-right"><span class="text-primary-text block font-bold">Next</span>
<a href=/posts/the-trending-of-cpu-management-in-k8s/ class=block>k8s中管理CPU资源管理的动态</a></div></div><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var t=document,e=t.createElement('script');e.async=!0,e.src='//edwardesire.disqus.com/embed.js',e.setAttribute('data-timestamp',+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><div class="lg:col-start-2 bg-secondary-bg prose col-span-2 rounded p-6 lg:col-span-6"><h3>See Also</h3><a href=/posts/the-trending-of-cpu-management-in-k8s/ class=no-underline>k8s中管理CPU资源管理的动态</a><br><a href=/posts/node-resource-topology-aware-scheduling/ class=no-underline>资源拓扑感知调度方案</a><br><a href=/posts/numa-aware-scheduling-in-volcano/ class=no-underline>Volcano的NUMA感知调度实现</a><br><a href=/posts/k8s-resources-topology-manager/ class=no-underline>k8s资源拓扑感知——资源分配</a><br><a href=/posts/multi-cluster-scheduling/ class=no-underline>多集群调度</a><br><a href=/posts/load-scheduling-brief/ class=no-underline>集群内负载调度方案调研</a><br></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.edwardesire.com/>Edward Desire</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>