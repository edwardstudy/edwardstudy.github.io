<!doctype html><html lang=en><meta charset=utf-8><meta name=viewport content="width=device-width"><title>k8s中管理CPU资源管理的动态 | Edwardesire</title><meta name=generator content="Hugo Eureka 0.8.3"><link rel=stylesheet href=/css/eureka.min.css><script defer src=/js/eureka.min.js></script>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap" as=style onload='this.onload=null,this.rel="stylesheet"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/dart.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js integrity="sha256-uNYoXefWRqv+PsIF/OflNmwtKM4lStn9yrz2gVl6ymo=" crossorigin></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js integrity="sha256-Zmpaaj+GXFsPF5WdPArSrnW3b30dovldeKsW00xBVwE=" crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=/images/favicon-32x32_hub39b788047e733a7d94c242c3bed8659_782_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=/images/favicon-32x32_hub39b788047e733a7d94c242c3bed8659_782_180x180_fill_box_center_3.png><meta name=description content="本文大致介绍k8s中CPU管理的现状与限制，以及社区相关的issues和提案。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":2,"name":"k8s中管理CPU资源管理的动态","item":"/posts/the-trending-of-cpu-management-in-k8s/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/the-trending-of-cpu-management-in-k8s/"},"headline":"k8s中管理CPU资源管理的动态 | Edwardesire","datePublished":"2022-03-23T10:38:03+08:00","dateModified":"2022-03-23T10:38:03+08:00","wordCount":6915,"publisher":{"@type":"Person","name":"C. Wang","logo":{"@type":"ImageObject","url":"/images/favicon-32x32.png"}},"description":"本文大致介绍k8s中CPU管理的现状与限制，以及社区相关的issues和提案。"}</script><meta property="og:title" content="k8s中管理CPU资源管理的动态 | Edwardesire"><meta property="og:type" content="article"><meta property="og:image" content="/images/favicon-32x32.png"><meta property="og:url" content="/posts/the-trending-of-cpu-management-in-k8s/"><meta property="og:description" content="本文大致介绍k8s中CPU管理的现状与限制，以及社区相关的issues和提案。"><meta property="og:locale" content="en"><meta property="og:site_name" content="Edwardesire"><meta property="article:published_time" content="2022-03-23T10:38:03+08:00"><meta property="article:modified_time" content="2022-03-23T10:38:03+08:00"><meta property="article:section" content="posts"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="kubelet"><meta property="article:tag" content="container"><meta property="og:see_also" content="/posts/node-resource-topology-aware-scheduling/"><meta property="og:see_also" content="/posts/numa-aware-scheduling-in-volcano/"><meta property="og:see_also" content="/posts/k8s-resources-topology-manager/"><meta property="og:see_also" content="/posts/multi-cluster-scheduling/"><meta property="og:see_also" content="/posts/load-scheduling-brief/"><meta property="og:see_also" content="/posts/jie-jue-kubernetes-admission-webhook-timeout-error/"><body class="flex flex-col min-h-screen"><header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm"><div class="w-full max-w-screen-xl mx-auto"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="mr-6 text-primary-text text-xl font-bold">Edwardesire</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item mr-4">Posts</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById("lightDarkMode");storageColorScheme==null||storageColorScheme=="Auto"?document.addEventListener("DOMContentLoaded",()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",switchDarkMode)}):storageColorScheme=="Light"?(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","sun"),element.firstElementChild.classList.add("fa-sun")):storageColorScheme=="Dark"&&(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","moon"),element.firstElementChild.classList.add("fa-moon")),document.addEventListener("DOMContentLoaded",()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="flex-grow pt-16"><div class=pl-scrollbar><div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto"><div class="grid grid-cols-2 lg:grid-cols-8 gap-4 lg:pt-12"><div class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded px-6 py-8"><h1 class="font-bold text-3xl text-primary-text">k8s中管理CPU资源管理的动态</h1><div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text"><div class="mr-6 my-2"><i class="fas fa-calendar mr-1"></i>
<span>2022-03-23</span></div><div class="mr-6 my-2"><i class="fas fa-clock mr-1"></i>
<span>14 min read</span></div><div class="mr-6 my-2"><i class="fas fa-folder mr-1"></i>
<a href=/categories/kubernetes/ class=hover:text-eureka>Kubernetes</a>
<span>,</span>
<a href=/categories/kubelet/ class=hover:text-eureka>kubelet</a>
<span>,</span>
<a href=/categories/container/ class=hover:text-eureka>container</a></div></div><div class=content><p>k8s的cpuManager完成节点侧的cpu资源分配和隔离（core pinning and isolation，如何做到隔离）。</p><ul><li>发现机器上的cpu拓扑</li><li>上报给k8s层机器的可用资源（包含kubelet侧的调度）</li><li>分配资源供workload执行</li><li>追踪pod的资源分配情况</li></ul><p>本文大致介绍k8s中CPU管理的现状与限制，结合社区文档分析当前社区动态。</p><ol><li>CPU管理的现状与限制</li><li>相关的issues</li><li>社区提案</li></ol><h2 id=cpu管理的现状与限制>CPU管理的现状与限制</h2><p>kubelet将系统的cpu分为2个资源池：</p><ul><li>独占池（exclusive pool）：同时只有一个任务能够分配到cpu</li><li>共享池（shared pool）：多个进程分配到cpu</li></ul><p>原生的k8s cpuManager目前只提供静态的cpu分配策略。当k8s创建一个pod后，pod会被分类为一个QoS：</p><ul><li>Guaranteed</li><li>Burstable</li><li>BestEffort</li></ul><p>并且kubelet允许管理员通过<code> –reserved-cpus</code>指定保留的CPU提供给系统进程或者kube守护进程（kubelet， npd）。保留的这部分资源主要提供给系统进程使用。可以作为共享池分配给非Guaranteed的pod容器。但是Guaranteed类pod无法分配这些cpus。</p><p>目前k8s的节点侧依据cpuManager的分配策略来分配numa node的cpuset，能够做到：</p><ul><li>容器被分配到一个numa node上。</li><li>容器被分配到一组共享的numa node上。</li></ul><p>cpuManager当前的限制：</p><ul><li>最大numa node数不能大于8，防止状态爆炸（state explosion）。</li><li>策略只支持静态分配cpuset，未来会支持在容器生命周期内动态调整cpuset。</li><li>调度器不感知节点上的拓扑信息。下文会介绍相应的提案。</li><li>对于线程布局（thread placement）的应用，防止物理核的共享和邻居干扰。cpu manager当前不支持。下文有介绍相应的提案。</li></ul><h2 id=相关issues>相关issues</h2><ol><li>针对处理器的异构特征，用户可以指定服务所需要的硬件类别。</li></ol><p><a href=https://github.com/kubernetes/kubernetes/issues/106157>https://github.com/kubernetes/kubernetes/issues/106157</a></p><p>异构计算的异构资源有着不同额性能和特征和多级。比如Intel 11th gen，性能内核（Performance-cores, P-cores）是高性能内核，效率内核（Efficiency-cores,）是性能功耗比更优的内核。</p><p>ref： <a href=https://www.intel.cn/content/www/cn/zh/gaming/resources/how-hybrid-design-works.html>https://www.intel.cn/content/www/cn/zh/gaming/resources/how-hybrid-design-works.html</a></p><p>这个issue描述的用户场景是，可以将E-cores分配给守护进程或者后台任务，将P-cores分配给性能要求更高的应用服务。支持这种场景需要对CPU进行分组分配。但是issue具体的方案讨论。因为底层硬件差异，目前无法做到通用。目前k8s层需要设计重构方案。</p><p>当前相关需求的落地方案都是在k8s上使用扩展资源的方式来标识不同的异构资源。这种方法会产生对于原生cpu/内存资源的重复统计。</p><ol start=2><li>topologyManager的best-effort策略优化</li></ol><p><a href=https://github.com/kubernetes/kubernetes/issues/106270>https://github.com/kubernetes/kubernetes/issues/106270</a></p><p>issue 提到best-effort的策略，迭代每个provider hint，依据位与运算聚合结果。如果最后的结果为not preferred，topologyManager应该尽力依据资源的倾向做到preferred的选择。这个想法的初衷是因为cpu资源相比其他外设的numa 亲和更重要。当多个provider hint相互冲突时，如果cpu有preferred的单numa node分配结果，应该先满足cpu的分配结果。比如cpu返回的结果为 [&lsquo;10&rsquo; preferred, &lsquo;11&rsquo; non-preferred])，一个设备返回的结果[&lsquo;01&rsquo;, preferred]。topologyManager应该使用'10&rsquo; preferred作为最后的结果，而不是合并之后的'01&rsquo; not preferred。</p><p>而社区对于这种的调度逻辑的改变，建议是创建新的policy以提供类似调度器优选（scoring）的算法系统。</p><ol start=3><li>严格的kubelet预留资源</li></ol><p><a href=https://github.com/kubernetes/kubernetes/issues/104147>https://github.com/kubernetes/kubernetes/issues/104147</a></p><p>希望提供新的参数StrictCPUReservation，表示严格的预留资源，DefaultCPUSet列表会移除ReservedSystemCPUs.</p><ol start=4><li>bug：释放init container的资源时，释放了重新分配给main container的资源。</li></ol><p><a href=https://github.com/kubernetes/kubernetes/issues/105114>https://github.com/kubernetes/kubernetes/issues/105114</a></p><p>这个issue已经修复：在RemoveContainer阶段，排除还在使用的容器的cpuset。剩下的cpuset才可以释放回DefaultCPUSet。</p><ol start=5><li>支持原地垂直扩展：针对已经部署到节点的pod实例，通过resize请求，修改pod的资源量。</li></ol><p><a href=https://github.com/kubernetes/enhancements/issues/1287>https://github.com/kubernetes/enhancements/issues/1287</a></p><p>原地垂直扩展的意思是：当业务调整服务的资源时，不需要重启容器。</p><p>原地垂直扩容是个复杂的功能，这里大致介绍设计思路。详细实现可以看PR: <a href=https://github.com/kubernetes/kubernetes/pull/102884>https://github.com/kubernetes/kubernetes/pull/102884</a>。</p><p>kube-scheduler依然使用pod的Spec&mldr;Resources.Requests来进行调度。依据pod的Status.Resize状态，判断缓存中node已经分配的资源量。</p><ul><li>Status.Resize = &ldquo;InProgress&rdquo; or &ldquo;Infeasible&rdquo;，依据Status&mldr;ResourcesAllocated（已经分配的值）统计资源量。</li><li>Status.Resize = &ldquo;Proposed&rdquo;，依据Spec&mldr;Resources.Requests（新修改的值） 和 Status&mldr;ResourcesAllocated（已经分配的值，如果resize合适，kubelet也会将新requests更新这个属性），取两者的最大值。</li></ul><p>kubelet侧的核心在admit阶段来判断剩余资源是否满足resize。而具体resize是否需要容器重启，需要依据container runtime来判断。所以这个resize功能其实是尽力型。通过ResizePolicy字段来判断：</p><p><img src=https://cdn.jsdelivr.net/gh/edwardstudy/static@latest/images/image-20220228172218634.png alt=ResourceResizePolicy></p><p>还值得注意点是当前PR主要是在kata、docker上支持原地重启，windows容器还未支持。</p><h2 id=有趣的社区提案>有趣的社区提案</h2><h3 id=调度器拓扑感知调度>调度器拓扑感知调度</h3><p>Redhat将他们实现的一套<a href=/posts/node-resource-topology-aware-scheduling/ title=node-resource-topology-aware-scheduling>拓扑调度</a>的方案贡献到社区：https://github.com/kubernetes/enhancements/pull/2787</p><h3 id=扩展cpumanager防止理核不在容器间共享>扩展cpuManager防止理核不在容器间共享：</h3><p>kep： <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2625-cpumanager-policies-thread-placement>https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2625-cpumanager-policies-thread-placement</a></p><p>防止同一个物理核的虚拟分配带来的干扰。</p><p>设计文档里引入新参数cpumanager-policy-options：full-pcpus-only，期望分配独占一个物理cpu。当指定了<code>full-pcpus-only</code>参数以及<code>static</code>策略时，cpuManager会在分配cpusets会额外检查，确保分配cpu的时候是分配整个物理核。从而确保容器在物理核上的竞争。</p><p>具体例子比如，一个容器申请了5个独占核（虚拟核），cpu 0-4都分配个了服务容器。cpu 5也被锁住不能再分配给容器。因为cpu 5和cpu 4同在一个物理核上。</p><p><img src=https://github.com/kubernetes/enhancements/raw/master/keps/sig-node/2625-cpumanager-policies-thread-placement/smtalign-allocation-odd-cores.png alt="Example core allocation with the full-pcpus-only policy option when requesting a odd number of cores"></p><ol start=3><li>增加cpuMananger跨numa分散策略：distribute-cpus-across-numa</li></ol><p><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2902-cpumanager-distribute-cpus-policy-option>https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2902-cpumanager-distribute-cpus-policy-option</a></p><ul><li><code>full-pcpus-only</code> ：上面已经描述：<code>full-pcpus-only</code>确保容器分配的cpu物理核独占 。</li><li><code>distribute-cpus-across-numa</code>：跨numa node 均匀分配容器。</li></ul><p>开启<code>distribute-cpus-across-numa</code>时，当容器需要分配跨numa node时，statie policy会跨numa node平均分配cpu。非开启的默认逻辑是优选填满一个numa node。防止跨numa node分配时，在一个余量最小的numa node上分配。从整个应用性能考虑，性能瓶颈收到落在剩余资源较少的numa node上性能最差的worker（process？）。这个选项能够提供整体性能。</p><hr><p>接下来介绍几个社区slack里讨论的几个提案：</p><ul><li>CPU Manager Plugin Model</li><li>Node Resource Interface</li><li>Dynamic resource allocation</li></ul><h3 id=cpu-manager-plugin-model>CPU Manager Plugin Model</h3><p>CPU Manager Plugin Model：kubelet cpuManager的插件框架。在不改动资源管理主流程前提下，支持不同的cpu分配场景。依据业务需求，实现更细粒度的控制cpuset。</p><p>kubelet在pod绑定成功之后，会将pod压入本地调度队列里，依次执行pod的cpuset的调度流程。调度流程本质上借鉴了kube-scheduler的调度框架。</p><p>插件可扩展点：</p><p>一个插件可以实现1个或多个可扩展点：</p><ul><li>Sort：将调度到节点上的pod排序处理。例如依据pod QoS判定的优先顺序。</li><li>Filter：过滤无法分配给pod的cpu。</li><li>PostFilter：当没有合适的cpu时，可以通过PostFilter进行预处理，然后将pod重新进行处理。</li><li>PreScore：对于单个cpu评分，提供给后面流程来判定分配组合的优先级。</li><li>Select：依据PreScore的结果选择一个cpu组合的最优解，最优解的结构是一组cpu。</li><li>Score：依据Select的结果——cpu分配组合评分。</li><li>Allocate：在分配cpuset之后，调用该插件。</li><li>Deallocate：在PostFilter之后，释放cpu的分配。</li></ul><p>三个评分插件的区别：</p><ul><li><p>PreScore：返回以cpu为key，value为单个cpu对于pod容器的亲和程度。</p></li><li><p>Select：依据插件的领域知识（比如同一个numa的cpu分配结构聚合），将cpu组合的分数聚合。返回是一组最佳cpu。</p></li><li><p>Score：依据所有的cpu组合，评分分配组合依据插件强约束逻辑。</p></li></ul><p>方案提出了两种扩展插件的方案。当前在kubelet的容器管理中，topologyManager主要完成下列事项：</p><ul><li>调度用hintProvider，获得各个子管理域的可分配情况</li><li>编排整体的拓扑分配决策</li><li>提供“scopes”和policies参数来影响整体策略</li></ul><p>其他子管理域的子manager（如cpuManager）作为hintProvider提供单个分配策略。在CPU Manager Plugin Model中，子manager作为模型插件接口提供原有功能。</p><p>方案1：扩展子manager，让topologyManager感知cpuset</p><p>通过当前的值回去numa node的分配扩展到能够针对单个cpuset的分配倾向。扩展插件以hint providers的形式执行，主流程不需要修改。</p><p>缺点：其他hintProvider（其他资源的分配）并不感知cpu信息，导致hintProvider的结果未参考cpu分配。最终聚合的结果不一定是最优解。</p><p>方案2：扩展 cpuManager为插件模型</p><p>topologyManager依然通过GetTopologyHints()和Allocate() 调用cpuManager，cpuManager内部进行扩展调度流程。具体的扩展方式可以通过引入新的policy配置，或者通过调度框架的方式直接扩展。</p><p>缺点：cpuManager的结果并不决定性的，topologyManager会结合其他hints来分配。</p><p>可以看到CPU Manager Plugin Model当前提案还出于非常原始的阶段，主要是Red Hat的人在推。并未在社区充分讨论。</p><h3 id=node-resource-interface>Node Resource Interface</h3><p>该方案来自containerd。主要是在CRI中扩展NRI插件。</p><h4 id=containerd>containerd</h4><p>containerd主要工作在平台和更底层的runtime之间。平台是指docker、k8s这类容器平台，runtime是指runc, kata等更底层的运行时。containerd在中间提供容器进程的管理，镜像的管理，文件系统快照以及元数据和依赖管理。下图是containerd架构总览图：</p><p><img src=https://cdn.jsdelivr.net/gh/edwardstudy/static@latest/images/image-20220307114443938.png alt=overview-containerd-2020></p><ul><li><p>client是用户交互的第一层，提供接口给调用方。</p></li><li><p>core定义了核心功能接口。所有的数据都通过core管理存储（metadata store），所有其他组件/插件不需要存储数据。</p></li><li><p>backend中的runtime负责通过不同shim与底层runtime打交道。</p></li><li><p>api层主要提供两大类gRPC 服务：image，runtime。提供了多种插件扩展。</p></li></ul><p>在CRI这一层，包含了CRI、CNI、NRI类型的插件接口：</p><p><img src=https://cdn.jsdelivr.net/gh/edwardstudy/static@latest/images/image-20220307115459360.png alt=cri-workflow></p><ul><li>CRI plugin：容器运行时接口插件，通过共享namespace、cgroups给pod下所有的容器，负责定义pod。</li><li>CNI plugin：容器网络接口插件，配置容器网络。当containerd创建第一个容器之后，通过namespace配置网络。</li><li>NRI plugin：节点资源接口插件，管理cgroups和拓扑。</li></ul><h5 id=nri>NRI</h5><p>NRI位于containerd架构中的CRI插件，提供一个在容器运行时级别来管理节点资源的插件框架。</p><p>cni可以用来解决批量计算，延迟敏感性服务的性能问题，以及满足服务SLA/SLO、优先级等用户需求。例如性能需求通过将容器的cpu分配同一个numa node，来保证numa内的内存调用。当然除了numa，还有CPU、L3 cache等资源拓扑亲和性。</p><p>当前kubelet的实现是通过cpuManager的处理对象只能是guaranteed类的pod， topologyManager通过cpuManager提供的hints实现资源分配。</p><p>kubelet当前也不适合处理多种需求的扩展，因为在kubelet增加细粒度的资源分配会导致kubelet和CRI的界限越来越模糊。而上述CRI内的插件，则是在CRI容器生命周期期间调用，适合做resoruce pinning和节点的拓扑的感知。并且在CRI内部做插件定义和迭代，可以做到上层kubernetes以最小代价来适配变化。</p><p>在容器生命周期中，CNI/NRI插件能够注入到容器初始化进程的Create和Start之间：</p><p>Create->NRI->Start</p><p>以官方例子<a href=https://github.com/containerd/nri/blob/main/examples/clearcfs/main.go>clearcfs</a>：在启动容器前，依据qos类型调用cgroup命令，cpu.cfs_quota_us 为-1 表示不设上限。</p><p>可以分析出NRI直接控制cgroup，所以能有更底层的资源分配方式。不过越接近底层，处理逻辑的复杂度也越高。</p><h3 id=dynamic-resource-allocation>Dynamic resource allocation</h3><p>KEP里翻到了这个动态资源分配，方案提供了一套新的k8s管理资源和设备资源的模型。核心思想和存储类型（storageclass）类似，通过挂载来实现具体设备资源的声明和消费，而不是通过request/limit来分配一定数量的设备资源。</p><p>用例：</p><ol><li>设备初始化：为workload配置设备。基于容器需求的配置，但是这部分配配置不应该直接暴露给容器。</li><li>设备清理：容器结束后清理设备参数/数据等信息.</li><li>Partial allocation：支持部分分配，一个设备共享多个容器。</li><li>optional allocation：支持容器声明软性(可选的) 资源请求。例如：GPU and crypto-offload engines设备的应用场景。</li><li>Over the Fabric devices：支持容器使用网络上的设备资源。</li></ol><p>动态资源分配的设计目的是提供更灵活控制、用户友好的api，资源管理插件化不需要重新构建k8s 组件。</p><p>通过定义动态资源分配的资源分配协议和gRPC接口来管理新定义k8s资源 ResourceClass和ResourceClaim：</p><ul><li>ResourceClass指定资源的驱动和驱动参数</li><li>ResourceClaim指定业务使用资源的实例</li></ul><p>立即分配和延迟分配：</p><ul><li>立即分配：ResourceClaim创建时就分配。对于稀缺资源的分配能够有效使用（allocating a resource is expensive）。但是没有保障由于其他资源（cpu，内存）导致节点无法调度。</li><li>延迟分配：调度成功才分配。能够处理立即分配带来的问题。</li></ul><p>调用流程</p><p><img src=https://github.com/kubernetes/enhancements/raw/03e4040eff63613ca04d80c3a6379fdfb1ac7362/keps/sig-node/3063-dynamic-resource-allocation/components.png alt=dynamic-resources-allocation></p><ol><li>用户创建 带有resourceClaimTemplate配置的pod。</li><li>资源声明 controller创建resourceClaim。</li><li>依据resourceClaim的spec中，立即分配（immediate allocation）和延迟分配（delayed allocation）处理。</li><li>立即分配：资源驱动controller发现resourceClaim的创建时并claim。</li><li>延迟分配：调度器首先处理，过滤不满足条件的节点，获得候选节点集。资源驱动再过滤一次候选节点集不符合要求的节点。</li><li>当资源驱动完成资源分配之后，调度器预留资源并绑定节点。</li><li>节点上的kubelet负责pod的执行和资源管理（调用驱动插件）。</li><li>当pod删除时，kubelet负责停止pod的容器，并回收资源（调用驱动插件）。</li><li>pod删除之后，gc会负责相应的resourceClaim删除。</li></ol><p>这块文档没有具体描述：在立即分配的场景中，如果没有调度器工作，resoruce driver controller来节点选择机制是怎么样的。</p><hr><h2 id=总结>总结</h2><p>可以看到未来社区会对kubelet容器管理做一次重构，来支持更复杂的业务场景。近期在cpu资源管理上会落地的调度器拓扑感知调度，和定制化的kubelet cpu分配策略。在上述的一些case中，有发展潜力的是NRI方案。</p><ul><li>支持定制化扩展，kubelet可以直接载入扩展配置无需修改自身代码。</li><li>通过与CRI交互，kubelet将部分复杂的cpu分配需求下放到runtime来处理。</li></ul><h3 id=参考文档>参考文档</h3><ul><li><p>CPU Management Kubelet Use Cases & Current State： <a href=https://docs.google.com/document/d/1U4jjRR7kw18Rllh-xpAaNTBcPsK5jl48ZAVo7KRqkJk/edit>https://docs.google.com/document/d/1U4jjRR7kw18Rllh-xpAaNTBcPsK5jl48ZAVo7KRqkJk/edit</a></p></li><li><p>NRI：https://github.com/containerd/nri</p></li><li><p>Dynamic resource allocation KEP：https://github.com/kubernetes/enhancements/pull/3064</p></li></ul></div><div class=my-4><a href=/tags/kubernetes/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#Kubernetes</a>
<a href=/tags/kubelet/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#kubelet</a>
<a href=/tags/container/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#container</a></div><div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t"><div></div><div class="md:text-right mt-4 md:mt-0"><span class="block font-bold">Next</span>
<a href=/posts/node-resource-topology-aware-scheduling/ class=block>资源拓扑感知调度方案</a></div></div><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//edwardesire.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><div class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded p-6"><h2 class="text-lg font-semibold mb-4">See Also</h2><div class=content><a href=/posts/node-resource-topology-aware-scheduling/>资源拓扑感知调度方案</a><br><a href=/posts/numa-aware-scheduling-in-volcano/>Volcano的NUMA感知调度实现</a><br><a href=/posts/k8s-resources-topology-manager/>k8s资源拓扑感知——资源分配</a><br><a href=/posts/multi-cluster-scheduling/>多集群调度</a><br><a href=/posts/load-scheduling-brief/>集群内负载调度方案调研</a><br><a href=/posts/jie-jue-kubernetes-admission-webhook-timeout-error/>解决Kubernetes admission webhook timeout error</a><br></div></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.initHighlightingOnLoad()})</script></div></div></main><footer class=pl-scrollbar><div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.edwardesire.com/>Edward Desire</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>